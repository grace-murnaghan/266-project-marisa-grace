{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/env-266/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, BartForQuestionAnswering, TrainingArguments, Trainer\n",
    "import project_utility as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape=(14756, 5)\n",
      "valid_df.shape=(4229, 5)\n",
      "test_df.shape=(2096, 5)\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df = utils.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "# based on example here: https://huggingface.co/docs/transformers/en/model_doc/bart#transformers.BartForQuestionAnswering.forward.example\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"valhalla/bart-large-finetuned-squadv1\")\n",
    "model = BartForQuestionAnswering.from_pretrained(\"valhalla/bart-large-finetuned-squadv1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.shared.weight\n",
      "model.encoder.embed_positions.weight\n",
      "model.encoder.layers.0.self_attn.k_proj.weight\n",
      "model.encoder.layers.0.self_attn.k_proj.bias\n",
      "model.encoder.layers.0.self_attn.v_proj.weight\n",
      "model.encoder.layers.0.self_attn.v_proj.bias\n",
      "model.encoder.layers.0.self_attn.q_proj.weight\n",
      "model.encoder.layers.0.self_attn.q_proj.bias\n",
      "model.encoder.layers.0.self_attn.out_proj.weight\n",
      "model.encoder.layers.0.self_attn.out_proj.bias\n",
      "model.encoder.layers.0.self_attn_layer_norm.weight\n",
      "model.encoder.layers.0.self_attn_layer_norm.bias\n",
      "model.encoder.layers.0.fc1.weight\n",
      "model.encoder.layers.0.fc1.bias\n",
      "model.encoder.layers.0.fc2.weight\n",
      "model.encoder.layers.0.fc2.bias\n",
      "model.encoder.layers.0.final_layer_norm.weight\n",
      "model.encoder.layers.0.final_layer_norm.bias\n",
      "model.encoder.layers.1.self_attn.k_proj.weight\n",
      "model.encoder.layers.1.self_attn.k_proj.bias\n",
      "model.encoder.layers.1.self_attn.v_proj.weight\n",
      "model.encoder.layers.1.self_attn.v_proj.bias\n",
      "model.encoder.layers.1.self_attn.q_proj.weight\n",
      "model.encoder.layers.1.self_attn.q_proj.bias\n",
      "model.encoder.layers.1.self_attn.out_proj.weight\n",
      "model.encoder.layers.1.self_attn.out_proj.bias\n",
      "model.encoder.layers.1.self_attn_layer_norm.weight\n",
      "model.encoder.layers.1.self_attn_layer_norm.bias\n",
      "model.encoder.layers.1.fc1.weight\n",
      "model.encoder.layers.1.fc1.bias\n",
      "model.encoder.layers.1.fc2.weight\n",
      "model.encoder.layers.1.fc2.bias\n",
      "model.encoder.layers.1.final_layer_norm.weight\n",
      "model.encoder.layers.1.final_layer_norm.bias\n",
      "model.encoder.layers.2.self_attn.k_proj.weight\n",
      "model.encoder.layers.2.self_attn.k_proj.bias\n",
      "model.encoder.layers.2.self_attn.v_proj.weight\n",
      "model.encoder.layers.2.self_attn.v_proj.bias\n",
      "model.encoder.layers.2.self_attn.q_proj.weight\n",
      "model.encoder.layers.2.self_attn.q_proj.bias\n",
      "model.encoder.layers.2.self_attn.out_proj.weight\n",
      "model.encoder.layers.2.self_attn.out_proj.bias\n",
      "model.encoder.layers.2.self_attn_layer_norm.weight\n",
      "model.encoder.layers.2.self_attn_layer_norm.bias\n",
      "model.encoder.layers.2.fc1.weight\n",
      "model.encoder.layers.2.fc1.bias\n",
      "model.encoder.layers.2.fc2.weight\n",
      "model.encoder.layers.2.fc2.bias\n",
      "model.encoder.layers.2.final_layer_norm.weight\n",
      "model.encoder.layers.2.final_layer_norm.bias\n",
      "model.encoder.layers.3.self_attn.k_proj.weight\n",
      "model.encoder.layers.3.self_attn.k_proj.bias\n",
      "model.encoder.layers.3.self_attn.v_proj.weight\n",
      "model.encoder.layers.3.self_attn.v_proj.bias\n",
      "model.encoder.layers.3.self_attn.q_proj.weight\n",
      "model.encoder.layers.3.self_attn.q_proj.bias\n",
      "model.encoder.layers.3.self_attn.out_proj.weight\n",
      "model.encoder.layers.3.self_attn.out_proj.bias\n",
      "model.encoder.layers.3.self_attn_layer_norm.weight\n",
      "model.encoder.layers.3.self_attn_layer_norm.bias\n",
      "model.encoder.layers.3.fc1.weight\n",
      "model.encoder.layers.3.fc1.bias\n",
      "model.encoder.layers.3.fc2.weight\n",
      "model.encoder.layers.3.fc2.bias\n",
      "model.encoder.layers.3.final_layer_norm.weight\n",
      "model.encoder.layers.3.final_layer_norm.bias\n",
      "model.encoder.layers.4.self_attn.k_proj.weight\n",
      "model.encoder.layers.4.self_attn.k_proj.bias\n",
      "model.encoder.layers.4.self_attn.v_proj.weight\n",
      "model.encoder.layers.4.self_attn.v_proj.bias\n",
      "model.encoder.layers.4.self_attn.q_proj.weight\n",
      "model.encoder.layers.4.self_attn.q_proj.bias\n",
      "model.encoder.layers.4.self_attn.out_proj.weight\n",
      "model.encoder.layers.4.self_attn.out_proj.bias\n",
      "model.encoder.layers.4.self_attn_layer_norm.weight\n",
      "model.encoder.layers.4.self_attn_layer_norm.bias\n",
      "model.encoder.layers.4.fc1.weight\n",
      "model.encoder.layers.4.fc1.bias\n",
      "model.encoder.layers.4.fc2.weight\n",
      "model.encoder.layers.4.fc2.bias\n",
      "model.encoder.layers.4.final_layer_norm.weight\n",
      "model.encoder.layers.4.final_layer_norm.bias\n",
      "model.encoder.layers.5.self_attn.k_proj.weight\n",
      "model.encoder.layers.5.self_attn.k_proj.bias\n",
      "model.encoder.layers.5.self_attn.v_proj.weight\n",
      "model.encoder.layers.5.self_attn.v_proj.bias\n",
      "model.encoder.layers.5.self_attn.q_proj.weight\n",
      "model.encoder.layers.5.self_attn.q_proj.bias\n",
      "model.encoder.layers.5.self_attn.out_proj.weight\n",
      "model.encoder.layers.5.self_attn.out_proj.bias\n",
      "model.encoder.layers.5.self_attn_layer_norm.weight\n",
      "model.encoder.layers.5.self_attn_layer_norm.bias\n",
      "model.encoder.layers.5.fc1.weight\n",
      "model.encoder.layers.5.fc1.bias\n",
      "model.encoder.layers.5.fc2.weight\n",
      "model.encoder.layers.5.fc2.bias\n",
      "model.encoder.layers.5.final_layer_norm.weight\n",
      "model.encoder.layers.5.final_layer_norm.bias\n",
      "model.encoder.layers.6.self_attn.k_proj.weight\n",
      "model.encoder.layers.6.self_attn.k_proj.bias\n",
      "model.encoder.layers.6.self_attn.v_proj.weight\n",
      "model.encoder.layers.6.self_attn.v_proj.bias\n",
      "model.encoder.layers.6.self_attn.q_proj.weight\n",
      "model.encoder.layers.6.self_attn.q_proj.bias\n",
      "model.encoder.layers.6.self_attn.out_proj.weight\n",
      "model.encoder.layers.6.self_attn.out_proj.bias\n",
      "model.encoder.layers.6.self_attn_layer_norm.weight\n",
      "model.encoder.layers.6.self_attn_layer_norm.bias\n",
      "model.encoder.layers.6.fc1.weight\n",
      "model.encoder.layers.6.fc1.bias\n",
      "model.encoder.layers.6.fc2.weight\n",
      "model.encoder.layers.6.fc2.bias\n",
      "model.encoder.layers.6.final_layer_norm.weight\n",
      "model.encoder.layers.6.final_layer_norm.bias\n",
      "model.encoder.layers.7.self_attn.k_proj.weight\n",
      "model.encoder.layers.7.self_attn.k_proj.bias\n",
      "model.encoder.layers.7.self_attn.v_proj.weight\n",
      "model.encoder.layers.7.self_attn.v_proj.bias\n",
      "model.encoder.layers.7.self_attn.q_proj.weight\n",
      "model.encoder.layers.7.self_attn.q_proj.bias\n",
      "model.encoder.layers.7.self_attn.out_proj.weight\n",
      "model.encoder.layers.7.self_attn.out_proj.bias\n",
      "model.encoder.layers.7.self_attn_layer_norm.weight\n",
      "model.encoder.layers.7.self_attn_layer_norm.bias\n",
      "model.encoder.layers.7.fc1.weight\n",
      "model.encoder.layers.7.fc1.bias\n",
      "model.encoder.layers.7.fc2.weight\n",
      "model.encoder.layers.7.fc2.bias\n",
      "model.encoder.layers.7.final_layer_norm.weight\n",
      "model.encoder.layers.7.final_layer_norm.bias\n",
      "model.encoder.layers.8.self_attn.k_proj.weight\n",
      "model.encoder.layers.8.self_attn.k_proj.bias\n",
      "model.encoder.layers.8.self_attn.v_proj.weight\n",
      "model.encoder.layers.8.self_attn.v_proj.bias\n",
      "model.encoder.layers.8.self_attn.q_proj.weight\n",
      "model.encoder.layers.8.self_attn.q_proj.bias\n",
      "model.encoder.layers.8.self_attn.out_proj.weight\n",
      "model.encoder.layers.8.self_attn.out_proj.bias\n",
      "model.encoder.layers.8.self_attn_layer_norm.weight\n",
      "model.encoder.layers.8.self_attn_layer_norm.bias\n",
      "model.encoder.layers.8.fc1.weight\n",
      "model.encoder.layers.8.fc1.bias\n",
      "model.encoder.layers.8.fc2.weight\n",
      "model.encoder.layers.8.fc2.bias\n",
      "model.encoder.layers.8.final_layer_norm.weight\n",
      "model.encoder.layers.8.final_layer_norm.bias\n",
      "model.encoder.layers.9.self_attn.k_proj.weight\n",
      "model.encoder.layers.9.self_attn.k_proj.bias\n",
      "model.encoder.layers.9.self_attn.v_proj.weight\n",
      "model.encoder.layers.9.self_attn.v_proj.bias\n",
      "model.encoder.layers.9.self_attn.q_proj.weight\n",
      "model.encoder.layers.9.self_attn.q_proj.bias\n",
      "model.encoder.layers.9.self_attn.out_proj.weight\n",
      "model.encoder.layers.9.self_attn.out_proj.bias\n",
      "model.encoder.layers.9.self_attn_layer_norm.weight\n",
      "model.encoder.layers.9.self_attn_layer_norm.bias\n",
      "model.encoder.layers.9.fc1.weight\n",
      "model.encoder.layers.9.fc1.bias\n",
      "model.encoder.layers.9.fc2.weight\n",
      "model.encoder.layers.9.fc2.bias\n",
      "model.encoder.layers.9.final_layer_norm.weight\n",
      "model.encoder.layers.9.final_layer_norm.bias\n",
      "model.encoder.layers.10.self_attn.k_proj.weight\n",
      "model.encoder.layers.10.self_attn.k_proj.bias\n",
      "model.encoder.layers.10.self_attn.v_proj.weight\n",
      "model.encoder.layers.10.self_attn.v_proj.bias\n",
      "model.encoder.layers.10.self_attn.q_proj.weight\n",
      "model.encoder.layers.10.self_attn.q_proj.bias\n",
      "model.encoder.layers.10.self_attn.out_proj.weight\n",
      "model.encoder.layers.10.self_attn.out_proj.bias\n",
      "model.encoder.layers.10.self_attn_layer_norm.weight\n",
      "model.encoder.layers.10.self_attn_layer_norm.bias\n",
      "model.encoder.layers.10.fc1.weight\n",
      "model.encoder.layers.10.fc1.bias\n",
      "model.encoder.layers.10.fc2.weight\n",
      "model.encoder.layers.10.fc2.bias\n",
      "model.encoder.layers.10.final_layer_norm.weight\n",
      "model.encoder.layers.10.final_layer_norm.bias\n",
      "model.encoder.layers.11.self_attn.k_proj.weight\n",
      "model.encoder.layers.11.self_attn.k_proj.bias\n",
      "model.encoder.layers.11.self_attn.v_proj.weight\n",
      "model.encoder.layers.11.self_attn.v_proj.bias\n",
      "model.encoder.layers.11.self_attn.q_proj.weight\n",
      "model.encoder.layers.11.self_attn.q_proj.bias\n",
      "model.encoder.layers.11.self_attn.out_proj.weight\n",
      "model.encoder.layers.11.self_attn.out_proj.bias\n",
      "model.encoder.layers.11.self_attn_layer_norm.weight\n",
      "model.encoder.layers.11.self_attn_layer_norm.bias\n",
      "model.encoder.layers.11.fc1.weight\n",
      "model.encoder.layers.11.fc1.bias\n",
      "model.encoder.layers.11.fc2.weight\n",
      "model.encoder.layers.11.fc2.bias\n",
      "model.encoder.layers.11.final_layer_norm.weight\n",
      "model.encoder.layers.11.final_layer_norm.bias\n",
      "model.encoder.layernorm_embedding.weight\n",
      "model.encoder.layernorm_embedding.bias\n",
      "model.decoder.embed_positions.weight\n",
      "model.decoder.layers.0.self_attn.k_proj.weight\n",
      "model.decoder.layers.0.self_attn.k_proj.bias\n",
      "model.decoder.layers.0.self_attn.v_proj.weight\n",
      "model.decoder.layers.0.self_attn.v_proj.bias\n",
      "model.decoder.layers.0.self_attn.q_proj.weight\n",
      "model.decoder.layers.0.self_attn.q_proj.bias\n",
      "model.decoder.layers.0.self_attn.out_proj.weight\n",
      "model.decoder.layers.0.self_attn.out_proj.bias\n",
      "model.decoder.layers.0.self_attn_layer_norm.weight\n",
      "model.decoder.layers.0.self_attn_layer_norm.bias\n",
      "model.decoder.layers.0.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.0.encoder_attn.k_proj.bias\n",
      "model.decoder.layers.0.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.0.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.0.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.0.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.0.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.0.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.0.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.0.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.0.fc1.weight\n",
      "model.decoder.layers.0.fc1.bias\n",
      "model.decoder.layers.0.fc2.weight\n",
      "model.decoder.layers.0.fc2.bias\n",
      "model.decoder.layers.0.final_layer_norm.weight\n",
      "model.decoder.layers.0.final_layer_norm.bias\n",
      "model.decoder.layers.1.self_attn.k_proj.weight\n",
      "model.decoder.layers.1.self_attn.k_proj.bias\n",
      "model.decoder.layers.1.self_attn.v_proj.weight\n",
      "model.decoder.layers.1.self_attn.v_proj.bias\n",
      "model.decoder.layers.1.self_attn.q_proj.weight\n",
      "model.decoder.layers.1.self_attn.q_proj.bias\n",
      "model.decoder.layers.1.self_attn.out_proj.weight\n",
      "model.decoder.layers.1.self_attn.out_proj.bias\n",
      "model.decoder.layers.1.self_attn_layer_norm.weight\n",
      "model.decoder.layers.1.self_attn_layer_norm.bias\n",
      "model.decoder.layers.1.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.1.encoder_attn.k_proj.bias\n",
      "model.decoder.layers.1.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.1.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.1.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.1.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.1.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.1.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.1.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.1.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.1.fc1.weight\n",
      "model.decoder.layers.1.fc1.bias\n",
      "model.decoder.layers.1.fc2.weight\n",
      "model.decoder.layers.1.fc2.bias\n",
      "model.decoder.layers.1.final_layer_norm.weight\n",
      "model.decoder.layers.1.final_layer_norm.bias\n",
      "model.decoder.layers.2.self_attn.k_proj.weight\n",
      "model.decoder.layers.2.self_attn.k_proj.bias\n",
      "model.decoder.layers.2.self_attn.v_proj.weight\n",
      "model.decoder.layers.2.self_attn.v_proj.bias\n",
      "model.decoder.layers.2.self_attn.q_proj.weight\n",
      "model.decoder.layers.2.self_attn.q_proj.bias\n",
      "model.decoder.layers.2.self_attn.out_proj.weight\n",
      "model.decoder.layers.2.self_attn.out_proj.bias\n",
      "model.decoder.layers.2.self_attn_layer_norm.weight\n",
      "model.decoder.layers.2.self_attn_layer_norm.bias\n",
      "model.decoder.layers.2.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.2.encoder_attn.k_proj.bias\n",
      "model.decoder.layers.2.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.2.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.2.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.2.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.2.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.2.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.2.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.2.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.2.fc1.weight\n",
      "model.decoder.layers.2.fc1.bias\n",
      "model.decoder.layers.2.fc2.weight\n",
      "model.decoder.layers.2.fc2.bias\n",
      "model.decoder.layers.2.final_layer_norm.weight\n",
      "model.decoder.layers.2.final_layer_norm.bias\n",
      "model.decoder.layers.3.self_attn.k_proj.weight\n",
      "model.decoder.layers.3.self_attn.k_proj.bias\n",
      "model.decoder.layers.3.self_attn.v_proj.weight\n",
      "model.decoder.layers.3.self_attn.v_proj.bias\n",
      "model.decoder.layers.3.self_attn.q_proj.weight\n",
      "model.decoder.layers.3.self_attn.q_proj.bias\n",
      "model.decoder.layers.3.self_attn.out_proj.weight\n",
      "model.decoder.layers.3.self_attn.out_proj.bias\n",
      "model.decoder.layers.3.self_attn_layer_norm.weight\n",
      "model.decoder.layers.3.self_attn_layer_norm.bias\n",
      "model.decoder.layers.3.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.3.encoder_attn.k_proj.bias\n",
      "model.decoder.layers.3.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.3.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.3.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.3.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.3.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.3.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.3.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.3.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.3.fc1.weight\n",
      "model.decoder.layers.3.fc1.bias\n",
      "model.decoder.layers.3.fc2.weight\n",
      "model.decoder.layers.3.fc2.bias\n",
      "model.decoder.layers.3.final_layer_norm.weight\n",
      "model.decoder.layers.3.final_layer_norm.bias\n",
      "model.decoder.layers.4.self_attn.k_proj.weight\n",
      "model.decoder.layers.4.self_attn.k_proj.bias\n",
      "model.decoder.layers.4.self_attn.v_proj.weight\n",
      "model.decoder.layers.4.self_attn.v_proj.bias\n",
      "model.decoder.layers.4.self_attn.q_proj.weight\n",
      "model.decoder.layers.4.self_attn.q_proj.bias\n",
      "model.decoder.layers.4.self_attn.out_proj.weight\n",
      "model.decoder.layers.4.self_attn.out_proj.bias\n",
      "model.decoder.layers.4.self_attn_layer_norm.weight\n",
      "model.decoder.layers.4.self_attn_layer_norm.bias\n",
      "model.decoder.layers.4.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.4.encoder_attn.k_proj.bias\n",
      "model.decoder.layers.4.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.4.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.4.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.4.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.4.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.4.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.4.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.4.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.4.fc1.weight\n",
      "model.decoder.layers.4.fc1.bias\n",
      "model.decoder.layers.4.fc2.weight\n",
      "model.decoder.layers.4.fc2.bias\n",
      "model.decoder.layers.4.final_layer_norm.weight\n",
      "model.decoder.layers.4.final_layer_norm.bias\n",
      "model.decoder.layers.5.self_attn.k_proj.weight\n",
      "model.decoder.layers.5.self_attn.k_proj.bias\n",
      "model.decoder.layers.5.self_attn.v_proj.weight\n",
      "model.decoder.layers.5.self_attn.v_proj.bias\n",
      "model.decoder.layers.5.self_attn.q_proj.weight\n",
      "model.decoder.layers.5.self_attn.q_proj.bias\n",
      "model.decoder.layers.5.self_attn.out_proj.weight\n",
      "model.decoder.layers.5.self_attn.out_proj.bias\n",
      "model.decoder.layers.5.self_attn_layer_norm.weight\n",
      "model.decoder.layers.5.self_attn_layer_norm.bias\n",
      "model.decoder.layers.5.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.5.encoder_attn.k_proj.bias\n",
      "model.decoder.layers.5.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.5.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.5.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.5.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.5.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.5.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.5.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.5.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.5.fc1.weight\n",
      "model.decoder.layers.5.fc1.bias\n",
      "model.decoder.layers.5.fc2.weight\n",
      "model.decoder.layers.5.fc2.bias\n",
      "model.decoder.layers.5.final_layer_norm.weight\n",
      "model.decoder.layers.5.final_layer_norm.bias\n",
      "model.decoder.layers.6.self_attn.k_proj.weight\n",
      "model.decoder.layers.6.self_attn.k_proj.bias\n",
      "model.decoder.layers.6.self_attn.v_proj.weight\n",
      "model.decoder.layers.6.self_attn.v_proj.bias\n",
      "model.decoder.layers.6.self_attn.q_proj.weight\n",
      "model.decoder.layers.6.self_attn.q_proj.bias\n",
      "model.decoder.layers.6.self_attn.out_proj.weight\n",
      "model.decoder.layers.6.self_attn.out_proj.bias\n",
      "model.decoder.layers.6.self_attn_layer_norm.weight\n",
      "model.decoder.layers.6.self_attn_layer_norm.bias\n",
      "model.decoder.layers.6.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.6.encoder_attn.k_proj.bias\n",
      "model.decoder.layers.6.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.6.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.6.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.6.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.6.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.6.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.6.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.6.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.6.fc1.weight\n",
      "model.decoder.layers.6.fc1.bias\n",
      "model.decoder.layers.6.fc2.weight\n",
      "model.decoder.layers.6.fc2.bias\n",
      "model.decoder.layers.6.final_layer_norm.weight\n",
      "model.decoder.layers.6.final_layer_norm.bias\n",
      "model.decoder.layers.7.self_attn.k_proj.weight\n",
      "model.decoder.layers.7.self_attn.k_proj.bias\n",
      "model.decoder.layers.7.self_attn.v_proj.weight\n",
      "model.decoder.layers.7.self_attn.v_proj.bias\n",
      "model.decoder.layers.7.self_attn.q_proj.weight\n",
      "model.decoder.layers.7.self_attn.q_proj.bias\n",
      "model.decoder.layers.7.self_attn.out_proj.weight\n",
      "model.decoder.layers.7.self_attn.out_proj.bias\n",
      "model.decoder.layers.7.self_attn_layer_norm.weight\n",
      "model.decoder.layers.7.self_attn_layer_norm.bias\n",
      "model.decoder.layers.7.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.7.encoder_attn.k_proj.bias\n",
      "model.decoder.layers.7.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.7.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.7.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.7.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.7.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.7.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.7.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.7.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.7.fc1.weight\n",
      "model.decoder.layers.7.fc1.bias\n",
      "model.decoder.layers.7.fc2.weight\n",
      "model.decoder.layers.7.fc2.bias\n",
      "model.decoder.layers.7.final_layer_norm.weight\n",
      "model.decoder.layers.7.final_layer_norm.bias\n",
      "model.decoder.layers.8.self_attn.k_proj.weight\n",
      "model.decoder.layers.8.self_attn.k_proj.bias\n",
      "model.decoder.layers.8.self_attn.v_proj.weight\n",
      "model.decoder.layers.8.self_attn.v_proj.bias\n",
      "model.decoder.layers.8.self_attn.q_proj.weight\n",
      "model.decoder.layers.8.self_attn.q_proj.bias\n",
      "model.decoder.layers.8.self_attn.out_proj.weight\n",
      "model.decoder.layers.8.self_attn.out_proj.bias\n",
      "model.decoder.layers.8.self_attn_layer_norm.weight\n",
      "model.decoder.layers.8.self_attn_layer_norm.bias\n",
      "model.decoder.layers.8.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.8.encoder_attn.k_proj.bias\n",
      "model.decoder.layers.8.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.8.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.8.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.8.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.8.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.8.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.8.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.8.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.8.fc1.weight\n",
      "model.decoder.layers.8.fc1.bias\n",
      "model.decoder.layers.8.fc2.weight\n",
      "model.decoder.layers.8.fc2.bias\n",
      "model.decoder.layers.8.final_layer_norm.weight\n",
      "model.decoder.layers.8.final_layer_norm.bias\n",
      "model.decoder.layers.9.self_attn.k_proj.weight\n",
      "model.decoder.layers.9.self_attn.k_proj.bias\n",
      "model.decoder.layers.9.self_attn.v_proj.weight\n",
      "model.decoder.layers.9.self_attn.v_proj.bias\n",
      "model.decoder.layers.9.self_attn.q_proj.weight\n",
      "model.decoder.layers.9.self_attn.q_proj.bias\n",
      "model.decoder.layers.9.self_attn.out_proj.weight\n",
      "model.decoder.layers.9.self_attn.out_proj.bias\n",
      "model.decoder.layers.9.self_attn_layer_norm.weight\n",
      "model.decoder.layers.9.self_attn_layer_norm.bias\n",
      "model.decoder.layers.9.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.9.encoder_attn.k_proj.bias\n",
      "model.decoder.layers.9.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.9.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.9.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.9.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.9.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.9.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.9.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.9.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.9.fc1.weight\n",
      "model.decoder.layers.9.fc1.bias\n",
      "model.decoder.layers.9.fc2.weight\n",
      "model.decoder.layers.9.fc2.bias\n",
      "model.decoder.layers.9.final_layer_norm.weight\n",
      "model.decoder.layers.9.final_layer_norm.bias\n",
      "model.decoder.layers.10.self_attn.k_proj.weight\n",
      "model.decoder.layers.10.self_attn.k_proj.bias\n",
      "model.decoder.layers.10.self_attn.v_proj.weight\n",
      "model.decoder.layers.10.self_attn.v_proj.bias\n",
      "model.decoder.layers.10.self_attn.q_proj.weight\n",
      "model.decoder.layers.10.self_attn.q_proj.bias\n",
      "model.decoder.layers.10.self_attn.out_proj.weight\n",
      "model.decoder.layers.10.self_attn.out_proj.bias\n",
      "model.decoder.layers.10.self_attn_layer_norm.weight\n",
      "model.decoder.layers.10.self_attn_layer_norm.bias\n",
      "model.decoder.layers.10.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.10.encoder_attn.k_proj.bias\n",
      "model.decoder.layers.10.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.10.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.10.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.10.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.10.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.10.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.10.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.10.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.10.fc1.weight\n",
      "model.decoder.layers.10.fc1.bias\n",
      "model.decoder.layers.10.fc2.weight\n",
      "model.decoder.layers.10.fc2.bias\n",
      "model.decoder.layers.10.final_layer_norm.weight\n",
      "model.decoder.layers.10.final_layer_norm.bias\n",
      "model.decoder.layers.11.self_attn.k_proj.weight\n",
      "model.decoder.layers.11.self_attn.k_proj.bias\n",
      "model.decoder.layers.11.self_attn.v_proj.weight\n",
      "model.decoder.layers.11.self_attn.v_proj.bias\n",
      "model.decoder.layers.11.self_attn.q_proj.weight\n",
      "model.decoder.layers.11.self_attn.q_proj.bias\n",
      "model.decoder.layers.11.self_attn.out_proj.weight\n",
      "model.decoder.layers.11.self_attn.out_proj.bias\n",
      "model.decoder.layers.11.self_attn_layer_norm.weight\n",
      "model.decoder.layers.11.self_attn_layer_norm.bias\n",
      "model.decoder.layers.11.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.11.encoder_attn.k_proj.bias\n",
      "model.decoder.layers.11.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.11.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.11.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.11.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.11.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.11.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.11.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.11.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.11.fc1.weight\n",
      "model.decoder.layers.11.fc1.bias\n",
      "model.decoder.layers.11.fc2.weight\n",
      "model.decoder.layers.11.fc2.bias\n",
      "model.decoder.layers.11.final_layer_norm.weight\n",
      "model.decoder.layers.11.final_layer_norm.bias\n",
      "model.decoder.layernorm_embedding.weight\n",
      "model.decoder.layernorm_embedding.bias\n",
      "qa_outputs.weight\n",
      "qa_outputs.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2096/2096 [17:02<00:00,  2.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>id</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>bart_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>some more detailed work has been done at natio...</td>\n",
       "      <td>The 9 percent reduction of rice in Bangladesh ...</td>\n",
       "      <td>14095</td>\n",
       "      <td>flooding damage and climate variability</td>\n",
       "      <td>514</td>\n",
       "      <td>flooding damage and climate variability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>some more detailed work has been done at natio...</td>\n",
       "      <td>What kind of model of Bangladesh was had been ...</td>\n",
       "      <td>14096</td>\n",
       "      <td>a dynamic economywide model</td>\n",
       "      <td>70</td>\n",
       "      <td>dynamic economywide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>some more detailed work has been done at natio...</td>\n",
       "      <td>What approach did Ahmed use to estimate how ch...</td>\n",
       "      <td>14097</td>\n",
       "      <td>a modelling approach</td>\n",
       "      <td>639</td>\n",
       "      <td>modelling approach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>extreme sea level height fluctuations are also...</td>\n",
       "      <td>Where  height fluctuations are large?</td>\n",
       "      <td>2843</td>\n",
       "      <td>extreme sea level height fluctuations are also...</td>\n",
       "      <td>0</td>\n",
       "      <td>to the north</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>extreme sea level height fluctuations are also...</td>\n",
       "      <td>How  non-tide sea levels are obtained?</td>\n",
       "      <td>2844</td>\n",
       "      <td>the non-tide sea levels are obtained by spectr...</td>\n",
       "      <td>167</td>\n",
       "      <td>by spectrally removing the tidal energy from ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  some more detailed work has been done at natio...   \n",
       "1  some more detailed work has been done at natio...   \n",
       "2  some more detailed work has been done at natio...   \n",
       "3  extreme sea level height fluctuations are also...   \n",
       "4  extreme sea level height fluctuations are also...   \n",
       "\n",
       "                                            question     id  \\\n",
       "0  The 9 percent reduction of rice in Bangladesh ...  14095   \n",
       "1  What kind of model of Bangladesh was had been ...  14096   \n",
       "2  What approach did Ahmed use to estimate how ch...  14097   \n",
       "3              Where  height fluctuations are large?   2843   \n",
       "4             How  non-tide sea levels are obtained?   2844   \n",
       "\n",
       "                                              answer  answer_start  \\\n",
       "0            flooding damage and climate variability           514   \n",
       "1                        a dynamic economywide model            70   \n",
       "2                               a modelling approach           639   \n",
       "3  extreme sea level height fluctuations are also...             0   \n",
       "4  the non-tide sea levels are obtained by spectr...           167   \n",
       "\n",
       "                                         bart_answer  \n",
       "0            flooding damage and climate variability  \n",
       "1                                dynamic economywide  \n",
       "2                                 modelling approach  \n",
       "3                                       to the north  \n",
       "4   by spectrally removing the tidal energy from ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iterate through test to get answer spans\n",
    "for i in tqdm(test_df.index):\n",
    "    question = test_df['question'][i]\n",
    "    text = test_df['context'][i]\n",
    "    \n",
    "    inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    answer_start_index = outputs.start_logits.argmax()\n",
    "    answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "    answer_decoded = tokenizer.decode(predict_answer_tokens, skip_special_tokens=True)\n",
    "    test_df.at[i, 'bart_answer'] = answer_decoded\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('question: The 9 percent reduction of rice in Bangladesh is attributed to '\n",
      " 'what two variables?')\n",
      "'label answer: flooding damage and climate variability'\n",
      "'BART answer:  flooding damage and climate variability'\n",
      "\n",
      "('question: What kind of model of Bangladesh was had been used to estimate '\n",
      " 'economic damages from historical climate variability and future '\n",
      " 'anthropogenic climate change?')\n",
      "'label answer: a dynamic economywide model'\n",
      "'BART answer:  dynamic economywide'\n",
      "\n",
      "('question: What approach did Ahmed use to estimate how changes in climate '\n",
      " 'variability might affect crop yields and poverty rates in Tanzania to the '\n",
      " 'early 2030s')\n",
      "'label answer: a modelling approach'\n",
      "'BART answer:  modelling approach'\n",
      "\n",
      "'question: Where  height fluctuations are large?'\n",
      "('label answer: extreme sea level height fluctuations are also larger to the '\n",
      " 'north, as a result of increasing storm intensities at the more northerly '\n",
      " 'coastal locations')\n",
      "'BART answer:  to the north'\n",
      "\n",
      "'question: How  non-tide sea levels are obtained?'\n",
      "('label answer: the non-tide sea levels are obtained by spectrally removing '\n",
      " 'the tidal energy from the hourly tide gauge records (bromirski et al. 2003 ')\n",
      "('BART answer:  by spectrally removing the tidal energy from the hourly tide '\n",
      " 'gauge records')\n",
      "\n",
      "('question: Where the co-occurrences of extreme waves and extreme sea level '\n",
      " 'heights are illustrated?')\n",
      "('label answer: the probabilities of the potentially important co-occurrences '\n",
      " \"of extreme waves and extreme sea level heights are illustrated for peak hs ' \"\n",
      " 's at noaa buoys near san francisco in fig. 9 the probability distribution in '\n",
      " 'fig. 9 shows the historical (1981 - present')\n",
      "'BART answer:  noaa buoys near san francisco'\n",
      "\n",
      "('question: Plant fossils from several sites in Wyoming, North America, '\n",
      " 'indicate what?')\n",
      "('label answer: 10,000-year interval at the paleocene-eocene boundary a '\n",
      " 'northern extension of floral ranges of ~ 450-2,200 km occurred')\n",
      "'BART answer:  a northern extension of floral ranges of ~ 450-2,200 km occurred'\n",
      "\n",
      "('question: Persistence therefore appears to have been the predominant '\n",
      " 'response in')\n",
      "\"label answer: plant fossil record for this warm interval in earth's history\"\n",
      "'BART answer:  plant fossil record'\n",
      "\n",
      "'question: what are the three adaptive features can be suggested'\n",
      "('label answer: first, many species had a much wider ecological tolerance than '\n",
      " 'is apparent from their presentday distributions, and thus they contain gene '\n",
      " 'variations that enable tolerance of much higher temperatures and water '\n",
      " 'stress. this is probably the case for many tropical species jaramillo et al. '\n",
      " '2010) whose ancestors evolved during the late cretaceous in much higher '\n",
      " 'temperatures and co2 levels. second and closely related, higher levels of '\n",
      " 'co2 induced an adaptive physiological response in the plants that enabled '\n",
      " 'their persistence. a combination of modeling and experiments')\n",
      "'BART answer:  at least three adaptive features can be suggested'\n",
      "\n",
      "('question: What is constructing fuel breaks around a vulnerable population of '\n",
      " 'a valued plant species to prevent extinction from climate-aggravated '\n",
      " 'wildfire? Exapmple?')\n",
      "('label answer: rescuing a highly valued and climate-vulnerable animal species '\n",
      " 'by captive propagation (e.g., california condor gymnogyps californianus '\n",
      " 'shaw]); prescribing methods otherwise socially undesired (e.g., '\n",
      " 'insecticides) to aggressively combat insect mortality that threatens '\n",
      " 'high-value resources (e.g., insectand pathogeninfected young bristlecone '\n",
      " 'pine pinus longaeva d.k. bailey] forests in the white mountains); requesting '\n",
      " 'more than otherwise allotted water rights to maintain a unique and '\n",
      " 'ecologically critical aquatic ecosystem (e.g., mono lake, california, '\n",
      " 'relative to water delivery to los angeles for human use); and aggressively '\n",
      " 'remov ing invasive species (box 19')\n",
      "'BART answer: adaptation'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in test_df.index[:10]:\n",
    "    pprint(f\"question: {test_df['question'][i]}\")\n",
    "    pprint(f\"label answer: {test_df['answer'][i]}\")\n",
    "    pprint(f\"BART answer: {test_df['bart_answer'][i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_parquet('test_bart_qa_scored.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge scores:\n",
      "{'rouge1': np.float64(0.4341106284336489),\n",
      " 'rouge2': np.float64(0.3796218872329144),\n",
      " 'rougeL': np.float64(0.4321849585581291),\n",
      " 'rougeLsum': np.float64(0.4325789489633455)}\n",
      "\n",
      "average semantic similarity:\n",
      "tensor(0.5995)\n"
     ]
    }
   ],
   "source": [
    "utils.evaluate_abstractive(test_df, 'bart_answer',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at z-dickson/bart-large-cnn-climate-change-summarization and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# fine tune BART for climate for extractive QA task (model is BART fine tuned for summarization)\n",
    "model_checkpoint = 'z-dickson/bart-large-cnn-climate-change-summarization'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = BartForQuestionAnswering.from_pretrained(model_checkpoint) # vs. conditional generation for abstractive tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data for training\n",
    "def preprocess_function(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        list(zip(examples['question'], examples['context'])),\n",
    "        padding='max_length',\n",
    "        max_length=1024, #BART max len=1024\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i in range(len(examples[\"answer\"])):\n",
    "        context = examples[\"context\"][i]\n",
    "        answer = examples[\"answer\"][i]\n",
    "        answer_start = examples[\"answer_start\"][i]\n",
    "\n",
    "        answer_end = answer_start + len(answer) - 1\n",
    "\n",
    "        start_token = tokenizer.encode(context[:answer_start], add_special_tokens=False)\n",
    "        end_token = tokenizer.encode(context[:answer_end + 1], add_special_tokens=False)\n",
    "\n",
    "        start_positions.append(len(start_token))\n",
    "        end_positions.append(len(end_token) - 1)\n",
    "\n",
    "    tokenized_inputs[\"start_positions\"] = start_positions\n",
    "    tokenized_inputs[\"end_positions\"] = end_positions\n",
    "\n",
    "    return tokenized_inputs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  61%|██████    | 9000/14756 [00:06<00:04, 1320.32 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1096 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 14756/14756 [00:11<00:00, 1337.96 examples/s]\n",
      "Map: 100%|██████████| 4229/4229 [00:03<00:00, 1360.58 examples/s]\n",
      "Map: 100%|██████████| 2096/2096 [00:01<00:00, 1253.93 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# apply preprocess function to datasets for tokenization\n",
    "tokenized_train = Dataset.from_pandas(train_df).map(preprocess_function, batched=True)\n",
    "tokenized_valid = Dataset.from_pandas(val_df).map(preprocess_function, batched=True)\n",
    "tokenized_test = Dataset.from_pandas(test_df).map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_train[9000]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint([i for i in train_df.iloc[9000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 2\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results/bart_qa',\n",
    "    evaluation_strategy='epoch',\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.shared.weight\n",
      "model.encoder.embed_positions.weight\n",
      "model.encoder.layers.0.self_attn.k_proj.weight\n",
      "model.encoder.layers.0.self_attn.k_proj.bias\n",
      "model.encoder.layers.0.self_attn.v_proj.weight\n",
      "model.encoder.layers.0.self_attn.v_proj.bias\n",
      "model.encoder.layers.0.self_attn.q_proj.weight\n",
      "model.encoder.layers.0.self_attn.q_proj.bias\n",
      "model.encoder.layers.0.self_attn.out_proj.weight\n",
      "model.encoder.layers.0.self_attn.out_proj.bias\n",
      "model.encoder.layers.0.self_attn_layer_norm.weight\n",
      "model.encoder.layers.0.self_attn_layer_norm.bias\n",
      "model.encoder.layers.0.fc1.weight\n",
      "model.encoder.layers.0.fc1.bias\n",
      "model.encoder.layers.0.fc2.weight\n",
      "model.encoder.layers.0.fc2.bias\n",
      "model.encoder.layers.0.final_layer_norm.weight\n",
      "model.encoder.layers.0.final_layer_norm.bias\n",
      "model.encoder.layers.1.self_attn.k_proj.weight\n",
      "model.encoder.layers.1.self_attn.k_proj.bias\n",
      "model.encoder.layers.1.self_attn.v_proj.weight\n",
      "model.encoder.layers.1.self_attn.v_proj.bias\n",
      "model.encoder.layers.1.self_attn.q_proj.weight\n",
      "model.encoder.layers.1.self_attn.q_proj.bias\n",
      "model.encoder.layers.1.self_attn.out_proj.weight\n",
      "model.encoder.layers.1.self_attn.out_proj.bias\n",
      "model.encoder.layers.1.self_attn_layer_norm.weight\n",
      "model.encoder.layers.1.self_attn_layer_norm.bias\n",
      "model.encoder.layers.1.fc1.weight\n",
      "model.encoder.layers.1.fc1.bias\n",
      "model.encoder.layers.1.fc2.weight\n",
      "model.encoder.layers.1.fc2.bias\n",
      "model.encoder.layers.1.final_layer_norm.weight\n",
      "model.encoder.layers.1.final_layer_norm.bias\n",
      "model.encoder.layers.2.self_attn.k_proj.weight\n",
      "model.encoder.layers.2.self_attn.k_proj.bias\n",
      "model.encoder.layers.2.self_attn.v_proj.weight\n",
      "model.encoder.layers.2.self_attn.v_proj.bias\n",
      "model.encoder.layers.2.self_attn.q_proj.weight\n",
      "model.encoder.layers.2.self_attn.q_proj.bias\n",
      "model.encoder.layers.2.self_attn.out_proj.weight\n",
      "model.encoder.layers.2.self_attn.out_proj.bias\n",
      "model.encoder.layers.2.self_attn_layer_norm.weight\n",
      "model.encoder.layers.2.self_attn_layer_norm.bias\n",
      "model.encoder.layers.2.fc1.weight\n",
      "model.encoder.layers.2.fc1.bias\n",
      "model.encoder.layers.2.fc2.weight\n",
      "model.encoder.layers.2.fc2.bias\n",
      "model.encoder.layers.2.final_layer_norm.weight\n",
      "model.encoder.layers.2.final_layer_norm.bias\n",
      "model.encoder.layers.3.self_attn.k_proj.weight\n",
      "model.encoder.layers.3.self_attn.k_proj.bias\n",
      "model.encoder.layers.3.self_attn.v_proj.weight\n",
      "model.encoder.layers.3.self_attn.v_proj.bias\n",
      "model.encoder.layers.3.self_attn.q_proj.weight\n",
      "model.encoder.layers.3.self_attn.q_proj.bias\n",
      "model.encoder.layers.3.self_attn.out_proj.weight\n",
      "model.encoder.layers.3.self_attn.out_proj.bias\n",
      "model.encoder.layers.3.self_attn_layer_norm.weight\n",
      "model.encoder.layers.3.self_attn_layer_norm.bias\n",
      "model.encoder.layers.3.fc1.weight\n",
      "model.encoder.layers.3.fc1.bias\n",
      "model.encoder.layers.3.fc2.weight\n",
      "model.encoder.layers.3.fc2.bias\n",
      "model.encoder.layers.3.final_layer_norm.weight\n",
      "model.encoder.layers.3.final_layer_norm.bias\n",
      "model.encoder.layers.4.self_attn.k_proj.weight\n",
      "model.encoder.layers.4.self_attn.k_proj.bias\n",
      "model.encoder.layers.4.self_attn.v_proj.weight\n",
      "model.encoder.layers.4.self_attn.v_proj.bias\n",
      "model.encoder.layers.4.self_attn.q_proj.weight\n",
      "model.encoder.layers.4.self_attn.q_proj.bias\n",
      "model.encoder.layers.4.self_attn.out_proj.weight\n",
      "model.encoder.layers.4.self_attn.out_proj.bias\n",
      "model.encoder.layers.4.self_attn_layer_norm.weight\n",
      "model.encoder.layers.4.self_attn_layer_norm.bias\n",
      "model.encoder.layers.4.fc1.weight\n",
      "model.encoder.layers.4.fc1.bias\n",
      "model.encoder.layers.4.fc2.weight\n",
      "model.encoder.layers.4.fc2.bias\n",
      "model.encoder.layers.4.final_layer_norm.weight\n",
      "model.encoder.layers.4.final_layer_norm.bias\n",
      "model.encoder.layers.5.self_attn.k_proj.weight\n",
      "model.encoder.layers.5.self_attn.k_proj.bias\n",
      "model.encoder.layers.5.self_attn.v_proj.weight\n",
      "model.encoder.layers.5.self_attn.v_proj.bias\n",
      "model.encoder.layers.5.self_attn.q_proj.weight\n",
      "model.encoder.layers.5.self_attn.q_proj.bias\n",
      "model.encoder.layers.5.self_attn.out_proj.weight\n",
      "model.encoder.layers.5.self_attn.out_proj.bias\n",
      "model.encoder.layers.5.self_attn_layer_norm.weight\n",
      "model.encoder.layers.5.self_attn_layer_norm.bias\n",
      "model.encoder.layers.5.fc1.weight\n",
      "model.encoder.layers.5.fc1.bias\n",
      "model.encoder.layers.5.fc2.weight\n",
      "model.encoder.layers.5.fc2.bias\n",
      "model.encoder.layers.5.final_layer_norm.weight\n",
      "model.encoder.layers.5.final_layer_norm.bias\n",
      "model.encoder.layers.6.self_attn.k_proj.weight\n",
      "model.encoder.layers.6.self_attn.k_proj.bias\n",
      "model.encoder.layers.6.self_attn.v_proj.weight\n",
      "model.encoder.layers.6.self_attn.v_proj.bias\n",
      "model.encoder.layers.6.self_attn.q_proj.weight\n",
      "model.encoder.layers.6.self_attn.q_proj.bias\n",
      "model.encoder.layers.6.self_attn.out_proj.weight\n",
      "model.encoder.layers.6.self_attn.out_proj.bias\n",
      "model.encoder.layers.6.self_attn_layer_norm.weight\n",
      "model.encoder.layers.6.self_attn_layer_norm.bias\n",
      "model.encoder.layers.6.fc1.weight\n",
      "model.encoder.layers.6.fc1.bias\n",
      "model.encoder.layers.6.fc2.weight\n",
      "model.encoder.layers.6.fc2.bias\n",
      "model.encoder.layers.6.final_layer_norm.weight\n",
      "model.encoder.layers.6.final_layer_norm.bias\n",
      "model.encoder.layers.7.self_attn.k_proj.weight\n",
      "model.encoder.layers.7.self_attn.k_proj.bias\n",
      "model.encoder.layers.7.self_attn.v_proj.weight\n",
      "model.encoder.layers.7.self_attn.v_proj.bias\n",
      "model.encoder.layers.7.self_attn.q_proj.weight\n",
      "model.encoder.layers.7.self_attn.q_proj.bias\n",
      "model.encoder.layers.7.self_attn.out_proj.weight\n",
      "model.encoder.layers.7.self_attn.out_proj.bias\n",
      "model.encoder.layers.7.self_attn_layer_norm.weight\n",
      "model.encoder.layers.7.self_attn_layer_norm.bias\n",
      "model.encoder.layers.7.fc1.weight\n",
      "model.encoder.layers.7.fc1.bias\n",
      "model.encoder.layers.7.fc2.weight\n",
      "model.encoder.layers.7.fc2.bias\n",
      "model.encoder.layers.7.final_layer_norm.weight\n",
      "model.encoder.layers.7.final_layer_norm.bias\n",
      "model.encoder.layers.8.self_attn.k_proj.weight\n",
      "model.encoder.layers.8.self_attn.k_proj.bias\n",
      "model.encoder.layers.8.self_attn.v_proj.weight\n",
      "model.encoder.layers.8.self_attn.v_proj.bias\n",
      "model.encoder.layers.8.self_attn.q_proj.weight\n",
      "model.encoder.layers.8.self_attn.q_proj.bias\n",
      "model.encoder.layers.8.self_attn.out_proj.weight\n",
      "model.encoder.layers.8.self_attn.out_proj.bias\n",
      "model.encoder.layers.8.self_attn_layer_norm.weight\n",
      "model.encoder.layers.8.self_attn_layer_norm.bias\n",
      "model.encoder.layers.8.fc1.weight\n",
      "model.encoder.layers.8.fc1.bias\n",
      "model.encoder.layers.8.fc2.weight\n",
      "model.encoder.layers.8.fc2.bias\n",
      "model.encoder.layers.8.final_layer_norm.weight\n",
      "model.encoder.layers.8.final_layer_norm.bias\n",
      "model.encoder.layers.9.self_attn.k_proj.weight\n",
      "model.encoder.layers.9.self_attn.k_proj.bias\n",
      "model.encoder.layers.9.self_attn.v_proj.weight\n",
      "model.encoder.layers.9.self_attn.v_proj.bias\n",
      "model.encoder.layers.9.self_attn.q_proj.weight\n",
      "model.encoder.layers.9.self_attn.q_proj.bias\n",
      "model.encoder.layers.9.self_attn.out_proj.weight\n",
      "model.encoder.layers.9.self_attn.out_proj.bias\n",
      "model.encoder.layers.9.self_attn_layer_norm.weight\n",
      "model.encoder.layers.9.self_attn_layer_norm.bias\n",
      "model.encoder.layers.9.fc1.weight\n",
      "model.encoder.layers.9.fc1.bias\n",
      "model.encoder.layers.9.fc2.weight\n",
      "model.encoder.layers.9.fc2.bias\n",
      "model.encoder.layers.9.final_layer_norm.weight\n",
      "model.encoder.layers.9.final_layer_norm.bias\n",
      "model.encoder.layers.10.self_attn.k_proj.weight\n",
      "model.encoder.layers.10.self_attn.k_proj.bias\n",
      "model.encoder.layers.10.self_attn.v_proj.weight\n",
      "model.encoder.layers.10.self_attn.v_proj.bias\n",
      "model.encoder.layers.10.self_attn.q_proj.weight\n",
      "model.encoder.layers.10.self_attn.q_proj.bias\n",
      "model.encoder.layers.10.self_attn.out_proj.weight\n",
      "model.encoder.layers.10.self_attn.out_proj.bias\n",
      "model.encoder.layers.10.self_attn_layer_norm.weight\n",
      "model.encoder.layers.10.self_attn_layer_norm.bias\n",
      "model.encoder.layers.10.fc1.weight\n",
      "model.encoder.layers.10.fc1.bias\n",
      "model.encoder.layers.10.fc2.weight\n",
      "model.encoder.layers.10.fc2.bias\n",
      "model.encoder.layers.10.final_layer_norm.weight\n",
      "model.encoder.layers.10.final_layer_norm.bias\n",
      "model.encoder.layers.11.self_attn.k_proj.weight\n",
      "model.encoder.layers.11.self_attn.k_proj.bias\n",
      "model.encoder.layers.11.self_attn.v_proj.weight\n",
      "model.encoder.layers.11.self_attn.v_proj.bias\n",
      "model.encoder.layers.11.self_attn.q_proj.weight\n",
      "model.encoder.layers.11.self_attn.q_proj.bias\n",
      "model.encoder.layers.11.self_attn.out_proj.weight\n",
      "model.encoder.layers.11.self_attn.out_proj.bias\n",
      "model.encoder.layers.11.self_attn_layer_norm.weight\n",
      "model.encoder.layers.11.self_attn_layer_norm.bias\n",
      "model.encoder.layers.11.fc1.weight\n",
      "model.encoder.layers.11.fc1.bias\n",
      "model.encoder.layers.11.fc2.weight\n",
      "model.encoder.layers.11.fc2.bias\n",
      "model.encoder.layers.11.final_layer_norm.weight\n",
      "model.encoder.layers.11.final_layer_norm.bias\n",
      "model.encoder.layernorm_embedding.weight\n",
      "model.encoder.layernorm_embedding.bias\n",
      "model.decoder.embed_positions.weight\n",
      "model.decoder.layers.0.self_attn.k_proj.weight\n",
      "model.decoder.layers.0.self_attn.k_proj.bias\n",
      "model.decoder.layers.0.self_attn.v_proj.weight\n",
      "model.decoder.layers.0.self_attn.v_proj.bias\n",
      "model.decoder.layers.0.self_attn.q_proj.weight\n",
      "model.decoder.layers.0.self_attn.q_proj.bias\n",
      "model.decoder.layers.0.self_attn.out_proj.weight\n",
      "model.decoder.layers.0.self_attn.out_proj.bias\n",
      "model.decoder.layers.0.self_attn_layer_norm.weight\n",
      "model.decoder.layers.0.self_attn_layer_norm.bias\n",
      "model.decoder.layers.0.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.0.encoder_attn.k_proj.bias\n",
      "model.decoder.layers.0.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.0.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.0.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.0.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.0.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.0.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.0.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.0.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.0.fc1.weight\n",
      "model.decoder.layers.0.fc1.bias\n",
      "model.decoder.layers.0.fc2.weight\n",
      "model.decoder.layers.0.fc2.bias\n",
      "model.decoder.layers.0.final_layer_norm.weight\n",
      "model.decoder.layers.0.final_layer_norm.bias\n",
      "model.decoder.layers.1.self_attn.k_proj.weight\n",
      "model.decoder.layers.1.self_attn.k_proj.bias\n",
      "model.decoder.layers.1.self_attn.v_proj.weight\n",
      "model.decoder.layers.1.self_attn.v_proj.bias\n",
      "model.decoder.layers.1.self_attn.q_proj.weight\n",
      "model.decoder.layers.1.self_attn.q_proj.bias\n",
      "model.decoder.layers.1.self_attn.out_proj.weight\n",
      "model.decoder.layers.1.self_attn.out_proj.bias\n",
      "model.decoder.layers.1.self_attn_layer_norm.weight\n",
      "model.decoder.layers.1.self_attn_layer_norm.bias\n",
      "model.decoder.layers.1.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.1.encoder_attn.k_proj.bias\n",
      "model.decoder.layers.1.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.1.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.1.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.1.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.1.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.1.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.1.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.1.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.1.fc1.weight\n",
      "model.decoder.layers.1.fc1.bias\n",
      "model.decoder.layers.1.fc2.weight\n",
      "model.decoder.layers.1.fc2.bias\n",
      "model.decoder.layers.1.final_layer_norm.weight\n",
      "model.decoder.layers.1.final_layer_norm.bias\n",
      "model.decoder.layers.2.self_attn.k_proj.weight\n",
      "model.decoder.layers.2.self_attn.k_proj.bias\n",
      "model.decoder.layers.2.self_attn.v_proj.weight\n",
      "model.decoder.layers.2.self_attn.v_proj.bias\n",
      "model.decoder.layers.2.self_attn.q_proj.weight\n",
      "model.decoder.layers.2.self_attn.q_proj.bias\n",
      "model.decoder.layers.2.self_attn.out_proj.weight\n",
      "model.decoder.layers.2.self_attn.out_proj.bias\n",
      "model.decoder.layers.2.self_attn_layer_norm.weight\n",
      "model.decoder.layers.2.self_attn_layer_norm.bias\n",
      "model.decoder.layers.2.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.2.encoder_attn.k_proj.bias\n",
      "model.decoder.layers.2.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.2.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.2.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.2.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.2.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.2.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.2.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.2.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.2.fc1.weight\n",
      "model.decoder.layers.2.fc1.bias\n",
      "model.decoder.layers.2.fc2.weight\n",
      "model.decoder.layers.2.fc2.bias\n",
      "model.decoder.layers.2.final_layer_norm.weight\n",
      "model.decoder.layers.2.final_layer_norm.bias\n",
      "model.decoder.layers.3.self_attn.k_proj.weight\n",
      "model.decoder.layers.3.self_attn.k_proj.bias\n",
      "model.decoder.layers.3.self_attn.v_proj.weight\n",
      "model.decoder.layers.3.self_attn.v_proj.bias\n",
      "model.decoder.layers.3.self_attn.q_proj.weight\n",
      "model.decoder.layers.3.self_attn.q_proj.bias\n",
      "model.decoder.layers.3.self_attn.out_proj.weight\n",
      "model.decoder.layers.3.self_attn.out_proj.bias\n",
      "model.decoder.layers.3.self_attn_layer_norm.weight\n",
      "model.decoder.layers.3.self_attn_layer_norm.bias\n",
      "model.decoder.layers.3.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.3.encoder_attn.k_proj.bias\n",
      "model.decoder.layers.3.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.3.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.3.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.3.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.3.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.3.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.3.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.3.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.3.fc1.weight\n",
      "model.decoder.layers.3.fc1.bias\n",
      "model.decoder.layers.3.fc2.weight\n",
      "model.decoder.layers.3.fc2.bias\n",
      "model.decoder.layers.3.final_layer_norm.weight\n",
      "model.decoder.layers.3.final_layer_norm.bias\n",
      "model.decoder.layers.4.self_attn.k_proj.weight\n",
      "model.decoder.layers.4.self_attn.k_proj.bias\n",
      "model.decoder.layers.4.self_attn.v_proj.weight\n",
      "model.decoder.layers.4.self_attn.v_proj.bias\n",
      "model.decoder.layers.4.self_attn.q_proj.weight\n",
      "model.decoder.layers.4.self_attn.q_proj.bias\n",
      "model.decoder.layers.4.self_attn.out_proj.weight\n",
      "model.decoder.layers.4.self_attn.out_proj.bias\n",
      "model.decoder.layers.4.self_attn_layer_norm.weight\n",
      "model.decoder.layers.4.self_attn_layer_norm.bias\n",
      "model.decoder.layers.4.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.4.encoder_attn.k_proj.bias\n",
      "model.decoder.layers.4.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.4.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.4.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.4.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.4.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.4.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.4.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.4.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.4.fc1.weight\n",
      "model.decoder.layers.4.fc1.bias\n",
      "model.decoder.layers.4.fc2.weight\n",
      "model.decoder.layers.4.fc2.bias\n",
      "model.decoder.layers.4.final_layer_norm.weight\n",
      "model.decoder.layers.4.final_layer_norm.bias\n",
      "model.decoder.layers.5.self_attn.k_proj.weight\n",
      "model.decoder.layers.5.self_attn.k_proj.bias\n",
      "model.decoder.layers.5.self_attn.v_proj.weight\n",
      "model.decoder.layers.5.self_attn.v_proj.bias\n",
      "model.decoder.layers.5.self_attn.q_proj.weight\n",
      "model.decoder.layers.5.self_attn.q_proj.bias\n",
      "model.decoder.layers.5.self_attn.out_proj.weight\n",
      "model.decoder.layers.5.self_attn.out_proj.bias\n",
      "model.decoder.layers.5.self_attn_layer_norm.weight\n",
      "model.decoder.layers.5.self_attn_layer_norm.bias\n",
      "model.decoder.layers.5.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.5.encoder_attn.k_proj.bias\n",
      "model.decoder.layers.5.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.5.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.5.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.5.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.5.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.5.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.5.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.5.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.5.fc1.weight\n",
      "model.decoder.layers.5.fc1.bias\n",
      "model.decoder.layers.5.fc2.weight\n",
      "model.decoder.layers.5.fc2.bias\n",
      "model.decoder.layers.5.final_layer_norm.weight\n",
      "model.decoder.layers.5.final_layer_norm.bias\n",
      "model.decoder.layers.6.self_attn.k_proj.weight\n",
      "model.decoder.layers.6.self_attn.k_proj.bias\n",
      "model.decoder.layers.6.self_attn.v_proj.weight\n",
      "model.decoder.layers.6.self_attn.v_proj.bias\n",
      "model.decoder.layers.6.self_attn.q_proj.weight\n",
      "model.decoder.layers.6.self_attn.q_proj.bias\n",
      "model.decoder.layers.6.self_attn.out_proj.weight\n",
      "model.decoder.layers.6.self_attn.out_proj.bias\n",
      "model.decoder.layers.6.self_attn_layer_norm.weight\n",
      "model.decoder.layers.6.self_attn_layer_norm.bias\n",
      "model.decoder.layers.6.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.6.encoder_attn.k_proj.bias\n",
      "model.decoder.layers.6.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.6.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.6.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.6.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.6.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.6.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.6.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.6.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.6.fc1.weight\n",
      "model.decoder.layers.6.fc1.bias\n",
      "model.decoder.layers.6.fc2.weight\n",
      "model.decoder.layers.6.fc2.bias\n",
      "model.decoder.layers.6.final_layer_norm.weight\n",
      "model.decoder.layers.6.final_layer_norm.bias\n",
      "model.decoder.layers.7.self_attn.k_proj.weight\n",
      "model.decoder.layers.7.self_attn.k_proj.bias\n",
      "model.decoder.layers.7.self_attn.v_proj.weight\n",
      "model.decoder.layers.7.self_attn.v_proj.bias\n",
      "model.decoder.layers.7.self_attn.q_proj.weight\n",
      "model.decoder.layers.7.self_attn.q_proj.bias\n",
      "model.decoder.layers.7.self_attn.out_proj.weight\n",
      "model.decoder.layers.7.self_attn.out_proj.bias\n",
      "model.decoder.layers.7.self_attn_layer_norm.weight\n",
      "model.decoder.layers.7.self_attn_layer_norm.bias\n",
      "model.decoder.layers.7.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.7.encoder_attn.k_proj.bias\n",
      "model.decoder.layers.7.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.7.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.7.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.7.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.7.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.7.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.7.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.7.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.7.fc1.weight\n",
      "model.decoder.layers.7.fc1.bias\n",
      "model.decoder.layers.7.fc2.weight\n",
      "model.decoder.layers.7.fc2.bias\n",
      "model.decoder.layers.7.final_layer_norm.weight\n",
      "model.decoder.layers.7.final_layer_norm.bias\n",
      "model.decoder.layers.8.self_attn.k_proj.weight\n",
      "model.decoder.layers.8.self_attn.k_proj.bias\n",
      "model.decoder.layers.8.self_attn.v_proj.weight\n",
      "model.decoder.layers.8.self_attn.v_proj.bias\n",
      "model.decoder.layers.8.self_attn.q_proj.weight\n",
      "model.decoder.layers.8.self_attn.q_proj.bias\n",
      "model.decoder.layers.8.self_attn.out_proj.weight\n",
      "model.decoder.layers.8.self_attn.out_proj.bias\n",
      "model.decoder.layers.8.self_attn_layer_norm.weight\n",
      "model.decoder.layers.8.self_attn_layer_norm.bias\n",
      "model.decoder.layers.8.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.8.encoder_attn.k_proj.bias\n",
      "model.decoder.layers.8.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.8.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.8.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.8.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.8.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.8.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.8.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.8.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.8.fc1.weight\n",
      "model.decoder.layers.8.fc1.bias\n",
      "model.decoder.layers.8.fc2.weight\n",
      "model.decoder.layers.8.fc2.bias\n",
      "model.decoder.layers.8.final_layer_norm.weight\n",
      "model.decoder.layers.8.final_layer_norm.bias\n",
      "model.decoder.layers.9.self_attn.k_proj.weight\n",
      "model.decoder.layers.9.self_attn.k_proj.bias\n",
      "model.decoder.layers.9.self_attn.v_proj.weight\n",
      "model.decoder.layers.9.self_attn.v_proj.bias\n",
      "model.decoder.layers.9.self_attn.q_proj.weight\n",
      "model.decoder.layers.9.self_attn.q_proj.bias\n",
      "model.decoder.layers.9.self_attn.out_proj.weight\n",
      "model.decoder.layers.9.self_attn.out_proj.bias\n",
      "model.decoder.layers.9.self_attn_layer_norm.weight\n",
      "model.decoder.layers.9.self_attn_layer_norm.bias\n",
      "model.decoder.layers.9.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.9.encoder_attn.k_proj.bias\n",
      "model.decoder.layers.9.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.9.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.9.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.9.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.9.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.9.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.9.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.9.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.9.fc1.weight\n",
      "model.decoder.layers.9.fc1.bias\n",
      "model.decoder.layers.9.fc2.weight\n",
      "model.decoder.layers.9.fc2.bias\n",
      "model.decoder.layers.9.final_layer_norm.weight\n",
      "model.decoder.layers.9.final_layer_norm.bias\n",
      "model.decoder.layers.10.self_attn.k_proj.weight\n",
      "model.decoder.layers.10.self_attn.k_proj.bias\n",
      "model.decoder.layers.10.self_attn.v_proj.weight\n",
      "model.decoder.layers.10.self_attn.v_proj.bias\n",
      "model.decoder.layers.10.self_attn.q_proj.weight\n",
      "model.decoder.layers.10.self_attn.q_proj.bias\n",
      "model.decoder.layers.10.self_attn.out_proj.weight\n",
      "model.decoder.layers.10.self_attn.out_proj.bias\n",
      "model.decoder.layers.10.self_attn_layer_norm.weight\n",
      "model.decoder.layers.10.self_attn_layer_norm.bias\n",
      "model.decoder.layers.10.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.10.encoder_attn.k_proj.bias\n",
      "model.decoder.layers.10.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.10.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.10.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.10.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.10.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.10.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.10.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.10.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.10.fc1.weight\n",
      "model.decoder.layers.10.fc1.bias\n",
      "model.decoder.layers.10.fc2.weight\n",
      "model.decoder.layers.10.fc2.bias\n",
      "model.decoder.layers.10.final_layer_norm.weight\n",
      "model.decoder.layers.10.final_layer_norm.bias\n",
      "model.decoder.layers.11.self_attn.k_proj.weight\n",
      "model.decoder.layers.11.self_attn.k_proj.bias\n",
      "model.decoder.layers.11.self_attn.v_proj.weight\n",
      "model.decoder.layers.11.self_attn.v_proj.bias\n",
      "model.decoder.layers.11.self_attn.q_proj.weight\n",
      "model.decoder.layers.11.self_attn.q_proj.bias\n",
      "model.decoder.layers.11.self_attn.out_proj.weight\n",
      "model.decoder.layers.11.self_attn.out_proj.bias\n",
      "model.decoder.layers.11.self_attn_layer_norm.weight\n",
      "model.decoder.layers.11.self_attn_layer_norm.bias\n",
      "model.decoder.layers.11.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.11.encoder_attn.k_proj.bias\n",
      "model.decoder.layers.11.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.11.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.11.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.11.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.11.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.11.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.11.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.11.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.11.fc1.weight\n",
      "model.decoder.layers.11.fc1.bias\n",
      "model.decoder.layers.11.fc2.weight\n",
      "model.decoder.layers.11.fc2.bias\n",
      "model.decoder.layers.11.final_layer_norm.weight\n",
      "model.decoder.layers.11.final_layer_norm.bias\n",
      "model.decoder.layernorm_embedding.weight\n",
      "model.decoder.layernorm_embedding.bias\n",
      "qa_outputs.weight\n",
      "qa_outputs.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_to_train = ['qa_outputs']\n",
    "for name, param in model.named_parameters():\n",
    "        if not any(layer in name for layer in layers_to_train):\n",
    "            param.requires_grad = False\n",
    "        if any(layer in name for layer in layers_to_train):\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train all layers\n",
    "# batch size = 2 , 2 epochs\n",
    "## max tokens = 1024 --> 120+ hours\n",
    "## max tokens = 512 --> 80 hours\n",
    "\n",
    "# train just qa outputs\n",
    "## batch size 4, 2 epochs\n",
    "## 1024 tokens --> ~5 hours\n",
    "## batch size 8, 2 epochs\n",
    "## 1024 tokens --> 4:50\n",
    "## batch size 32, 2 epochs, 1024 tokens --> 5 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='924' max='924' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [924/924 15:22:41, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.626285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.898200</td>\n",
       "      <td>5.578084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/env-266/lib/python3.11/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/env-266/lib/python3.11/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=924, training_loss=5.767886207217262, metrics={'train_runtime': 55383.1316, 'train_samples_per_second': 0.533, 'train_steps_per_second': 0.017, 'total_flos': 6.395596272707174e+16, 'train_loss': 5.767886207217262, 'epoch': 2.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/env-266/lib/python3.11/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint_filepath = './checkpoints/climate_bart'\n",
    "model.save_pretrained(model_checkpoint_filepath, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint_filepath = './checkpoints/climate_bart'\n",
    "model = BartForQuestionAnswering.from_pretrained(model_checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2096/2096 [31:06<00:00,  1.12it/s]    \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>id</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>climate_bart_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>some more detailed work has been done at natio...</td>\n",
       "      <td>The 9 percent reduction of rice in Bangladesh ...</td>\n",
       "      <td>14095</td>\n",
       "      <td>flooding damage and climate variability</td>\n",
       "      <td>514</td>\n",
       "      <td>The 9 percent reduction of rice in Bangladesh ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>some more detailed work has been done at natio...</td>\n",
       "      <td>What kind of model of Bangladesh was had been ...</td>\n",
       "      <td>14096</td>\n",
       "      <td>a dynamic economywide model</td>\n",
       "      <td>70</td>\n",
       "      <td>What kind of model of Bangladesh was had been ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>some more detailed work has been done at natio...</td>\n",
       "      <td>What approach did Ahmed use to estimate how ch...</td>\n",
       "      <td>14097</td>\n",
       "      <td>a modelling approach</td>\n",
       "      <td>639</td>\n",
       "      <td>What approach did Ahmed use to estimate how ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>extreme sea level height fluctuations are also...</td>\n",
       "      <td>Where  height fluctuations are large?</td>\n",
       "      <td>2843</td>\n",
       "      <td>extreme sea level height fluctuations are also...</td>\n",
       "      <td>0</td>\n",
       "      <td>Where  height fluctuations are large?extreme s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>extreme sea level height fluctuations are also...</td>\n",
       "      <td>How  non-tide sea levels are obtained?</td>\n",
       "      <td>2844</td>\n",
       "      <td>the non-tide sea levels are obtained by spectr...</td>\n",
       "      <td>167</td>\n",
       "      <td>How  non-tide sea levels are obtained?extreme ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  some more detailed work has been done at natio...   \n",
       "1  some more detailed work has been done at natio...   \n",
       "2  some more detailed work has been done at natio...   \n",
       "3  extreme sea level height fluctuations are also...   \n",
       "4  extreme sea level height fluctuations are also...   \n",
       "\n",
       "                                            question     id  \\\n",
       "0  The 9 percent reduction of rice in Bangladesh ...  14095   \n",
       "1  What kind of model of Bangladesh was had been ...  14096   \n",
       "2  What approach did Ahmed use to estimate how ch...  14097   \n",
       "3              Where  height fluctuations are large?   2843   \n",
       "4             How  non-tide sea levels are obtained?   2844   \n",
       "\n",
       "                                              answer  answer_start  \\\n",
       "0            flooding damage and climate variability           514   \n",
       "1                        a dynamic economywide model            70   \n",
       "2                               a modelling approach           639   \n",
       "3  extreme sea level height fluctuations are also...             0   \n",
       "4  the non-tide sea levels are obtained by spectr...           167   \n",
       "\n",
       "                                 climate_bart_answer  \n",
       "0  The 9 percent reduction of rice in Bangladesh ...  \n",
       "1  What kind of model of Bangladesh was had been ...  \n",
       "2  What approach did Ahmed use to estimate how ch...  \n",
       "3  Where  height fluctuations are large?extreme s...  \n",
       "4  How  non-tide sea levels are obtained?extreme ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in tqdm(test_df.index):\n",
    "    question = test_df['question'][i]\n",
    "    text = test_df['context'][i]\n",
    "    \n",
    "    inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    answer_start_index = outputs.start_logits.argmax()\n",
    "    answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "    answer_decoded = tokenizer.decode(predict_answer_tokens, skip_special_tokens=True)\n",
    "    test_df.at[i, 'climate_bart_answer'] = answer_decoded\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge scores:\n",
      "{'rouge1': np.float64(0.3248086876030988),\n",
      " 'rouge2': np.float64(0.2323292845317699),\n",
      " 'rougeL': np.float64(0.28354570895180087),\n",
      " 'rougeLsum': np.float64(0.283143466634203)}\n",
      "\n",
      "average semantic similarity:\n",
      "tensor(0.6485)\n"
     ]
    }
   ],
   "source": [
    "utils.evaluate_abstractive(test_df,'climate_bart_answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in test_df.index[:5]:\n",
    "#     print(test_df.iloc[i]['question'])\n",
    "#     print(test_df.iloc[i]['answer'])\n",
    "#     print(test_df.iloc[i]['climate_bart_answer'])\n",
    "#     print()\n",
    "\n",
    "# output is restating question and giving a varying span of the context - maybe train for a more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n",
      "/opt/miniconda3/envs/env-266/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='462' max='462' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [462/462 6:19:37, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.525818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/env-266/lib/python3.11/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=462, training_loss=5.5678594680059526, metrics={'train_runtime': 22797.7494, 'train_samples_per_second': 0.647, 'train_steps_per_second': 0.02, 'total_flos': 3.197798136353587e+16, 'train_loss': 5.5678594680059526, 'epoch': 1.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try training for another epoch to see if that improves the output\n",
    "model_checkpoint_filepath = './checkpoints/climate_bart'\n",
    "model = BartForQuestionAnswering.from_pretrained(model_checkpoint_filepath)\n",
    "\n",
    "layers_to_train = ['qa_outputs']\n",
    "for name, param in model.named_parameters():\n",
    "        if not any(layer in name for layer in layers_to_train):\n",
    "            param.requires_grad = False\n",
    "        if any(layer in name for layer in layers_to_train):\n",
    "            param.requires_grad = True\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 1\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results/bart_qa',\n",
    "    evaluation_strategy='epoch',\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/env-266/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='924' max='924' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [924/924 18:04:17, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.482283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.523600</td>\n",
       "      <td>5.472097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/env-266/lib/python3.11/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/env-266/lib/python3.11/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=924, training_loss=5.506216073965097, metrics={'train_runtime': 65078.4976, 'train_samples_per_second': 0.453, 'train_steps_per_second': 0.014, 'total_flos': 6.395596272707174e+16, 'train_loss': 5.506216073965097, 'epoch': 2.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 2\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results/bart_qa',\n",
    "    evaluation_strategy='epoch',\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/env-266/lib/python3.11/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint_filepath = './checkpoints/climate_bart/epoch5/'\n",
    "model.save_pretrained(model_checkpoint_filepath, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2096/2096 [17:51<00:00,  1.96it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge scores:\n",
      "{'rouge1': np.float64(0.3263647001615847),\n",
      " 'rouge2': np.float64(0.2345149501537028),\n",
      " 'rougeL': np.float64(0.28361609940254884),\n",
      " 'rougeLsum': np.float64(0.2838430615051699)}\n",
      "\n",
      "average semantic similarity:\n",
      "tensor(0.6534)\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "for i in tqdm(test_df.index):\n",
    "    question = test_df['question'][i]\n",
    "    text = test_df['context'][i]\n",
    "    \n",
    "    inputs = tokenizer(question, text, return_tensors=\"pt\").to('mps')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    answer_start_index = outputs.start_logits.argmax()\n",
    "    answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "    answer_decoded = tokenizer.decode(predict_answer_tokens, skip_special_tokens=True)\n",
    "    test_df.at[i, 'climate_bart_answer'] = answer_decoded\n",
    "\n",
    "utils.evaluate_abstractive(test_df,'climate_bart_answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 9 percent reduction of rice in Bangladesh is attributed to what two variables?\n",
      "flooding damage and climate variability\n",
      "The 9 percent reduction of rice in Bangladesh is attributed to what two variables?some more detailed work has been done at national level. for example, a dynamic economywide model of bangladesh has been used to estimate economic damages from historical climate variability and future anthropogenic climate change. using a combination of historical yield variability and ten climate projections, future anthropogenic climate change damages are estimated to reduce national rice production in bangladesh by about 9 percent to mid\n",
      "\n",
      "What kind of model of Bangladesh was had been used to estimate economic damages from historical climate variability and future anthropogenic climate change?\n",
      "a dynamic economywide model\n",
      "What kind of model of Bangladesh was had been used to estimate economic damages from historical climate variability and future anthropogenic climate change?some more detailed work has been done at national level. for example, a dynamic economywide model of bangladesh has been used to estimate economic damages from historical climate variability and future anthropogenic climate change. using a combination of historical yield variability and ten climate projections, future anthropogenic climate change damages are estimated to reduce national rice production in bangladesh by about 9 percent to mid-century, and most of these losses are attributed in the analysis to flooding damage and climate variability (thurlow et al., 2011). another example is the work of ahmed et al. (2011), who used a modelling approach to estimate\n",
      "\n",
      "What approach did Ahmed use to estimate how changes in climate variability might affect crop yields and poverty rates in Tanzania to the early 2030s\n",
      "a modelling approach\n",
      "What approach did Ahmed use to estimate how changes in climate variability might affect crop yields and poverty rates in Tanzania to the early 2030ssome more detailed work has been done at national level. for example, a dynamic economywide model of bangladesh has been used to estimate economic damages from historical climate variability and future anthropogenic climate change. using a combination of historical yield variability and ten climate projections, future anthropogenic climate change damages are estimated to reduce national rice production in bangladesh by about 9 percent to mid\n",
      "\n",
      "Where  height fluctuations are large?\n",
      "extreme sea level height fluctuations are also larger to the north, as a result of increasing storm intensities at the more northerly coastal locations\n",
      "Where  height fluctuations are large?extreme sea level height fluctuations are also larger to the north, as a result of increasing storm intensities at the more northerly coastal locations (fig. 8 left). the non-tide sea levels are obtained by spectrally removing the tidal energy from the hourly tide gauge records (bromirski et al. 2003 ). note that, for example, a 30 cm event is much less likely near san diego (sio) than near either san francisco (sfo) or crescent city (cre).\n",
      "\n",
      "How  non-tide sea levels are obtained?\n",
      "the non-tide sea levels are obtained by spectrally removing the tidal energy from the hourly tide gauge records (bromirski et al. 2003 \n",
      "How  non-tide sea levels are obtained?extreme sea level height fluctuations are also larger to the north, as a result of increasing storm intensities at the more northerly coastal locations (fig. 8 left). the non-tide sea levels are obtained by spectrally removing the tidal energy from the hourly tide gauge records (bromirski et al. 2003 ). note that, for example, a 30 cm event is much less likely near san diego (sio) than near either san francisco (sfo) or crescent city (cre). the probabilities of the potentially important co-occurrences of extreme waves\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in test_df.index[:5]:\n",
    "    print(test_df.iloc[i]['question'])\n",
    "    print(test_df.iloc[i]['answer'])\n",
    "    print(test_df.iloc[i]['climate_bart_answer'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BART is a seq2seq model so we could maybe use it out of the box for abstractive?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-266",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
