{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/cortex-installs/miniconda/envs/w266/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, BartForConditionalGeneration, BartForQuestionAnswering,TrainingArguments, Trainer\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "def prepare_data(data):\n",
    "    articles = []\n",
    "\n",
    "    for paragraph in data:\n",
    "        context = paragraph['context']\n",
    "        for qa in paragraph['qas']:\n",
    "            question = qa['question']\n",
    "            id = qa['id']\n",
    "            for ans in qa['answers']:\n",
    "                answer = ans['text']\n",
    "                answer_start = ans['answer_start']\n",
    "                articles.append({'context': context, 'question': question, 'id': id, 'answer': answer, 'answer_start': answer_start})\n",
    "\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape=(14756, 5)\n",
      "valid_df.shape=(4229, 5)\n",
      "test_df.shape=(2096, 5)\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"rony/climate-change-MRC\")\n",
    "train_ds = ds[\"train\"]\n",
    "valid_ds = ds[\"validation\"]\n",
    "test_ds = ds[\"test\"]\n",
    "\n",
    "# each is a 1-item list, so take first index\n",
    "train_ds = train_ds[0]\n",
    "valid_ds = valid_ds[0]\n",
    "test_ds = test_ds[0]\n",
    "\n",
    "# take the 'data' key of the dict, ignoring 'version' (there's just one)\n",
    "train_ds = train_ds['data'][0]['paragraphs']\n",
    "valid_ds = valid_ds['data'][0]['paragraphs']\n",
    "test_ds = test_ds['data'][0]['paragraphs']\n",
    "# each dataset is a list of dicts, where each list item is a context paragraph ('context' key) with qas ('qas' key) which contain questions, id, and answer\n",
    "\n",
    "train_df = pd.DataFrame(prepare_data(train_ds))\n",
    "print(f\"{train_df.shape=}\")\n",
    "\n",
    "valid_df = pd.DataFrame(prepare_data(valid_ds))\n",
    "print(f\"{valid_df.shape=}\")\n",
    "\n",
    "test_df = pd.DataFrame(prepare_data(test_ds))\n",
    "print(f\"{test_df.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels will be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels will be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels will be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels will be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "# fine tune BART for climate for extractive QA task (model is BART fine tuned for summarization)\n",
    "model_checkpoint = 'valhalla/bart-large-finetuned-squadv1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = BartForQuestionAnswering.from_pretrained(model_checkpoint) # vs. conditional generation for abstractive tasks\n",
    "\n",
    "lr = 1e-5\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "# LORA hyperparameters\n",
    "r = 8\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9000/14756 [00:11<00:07, 755.63 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1096 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14756/14756 [00:19<00:00, 743.30 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4229/4229 [00:05<00:00, 775.43 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2096/2096 [00:02<00:00, 754.25 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# preprocess data for training\n",
    "def preprocess_function(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "    list(zip(examples['question'], examples['context'])),\n",
    "    padding='max_length',\n",
    "    max_length=1024, #BART max len=1024\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i in range(len(examples[\"answer\"])):\n",
    "        context = examples[\"context\"][i]\n",
    "        answer = examples[\"answer\"][i]\n",
    "        answer_start = examples[\"answer_start\"][i]\n",
    "\n",
    "        answer_end = answer_start + len(answer) - 1\n",
    "\n",
    "        start_token = tokenizer.encode(context[:answer_start], add_special_tokens=False)\n",
    "        end_token = tokenizer.encode(context[:answer_end + 1], add_special_tokens=False)\n",
    "\n",
    "        start_positions.append(len(start_token))\n",
    "        end_positions.append(len(end_token) - 1)\n",
    "\n",
    "    tokenized_inputs[\"start_positions\"] = start_positions\n",
    "    tokenized_inputs[\"end_positions\"] = end_positions\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "# apply preprocess function to datasets for tokenization\n",
    "tokenized_train = Dataset.from_pandas(train_df).map(preprocess_function, batched=True)\n",
    "tokenized_valid = Dataset.from_pandas(valid_df).map(preprocess_function, batched=True)\n",
    "tokenized_test = Dataset.from_pandas(test_df).map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,181,698 || all params: 407,475,204 || trainable%: 0.2900\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "task_type=TaskType.QUESTION_ANS, inference_mode=False, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "_= model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/cortex-installs/miniconda/envs/w266/lib/python3.11/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "output_dir=\"bart_squad_ft_climate\",\n",
    "learning_rate=lr,\n",
    "per_device_train_batch_size=BATCH_SIZE,\n",
    "per_device_eval_batch_size=BATCH_SIZE,\n",
    "num_train_epochs=NUM_EPOCHS,\n",
    "weight_decay=0.01,\n",
    "evaluation_strategy=\"epoch\",\n",
    "save_strategy=\"epoch\",\n",
    "logging_steps=10,\n",
    "load_best_model_at_end=True,\n",
    "report_to='none',\n",
    "label_names=['start_positions', 'end_positions'],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "model=model,\n",
    "args=training_args,\n",
    "train_dataset=tokenized_train,\n",
    "eval_dataset=tokenized_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7378' max='7378' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7378/7378 5:24:32, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.101800</td>\n",
       "      <td>3.975826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.946500</td>\n",
       "      <td>3.895590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels will be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels will be overwritten to 2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7378, training_loss=4.276153972395134, metrics={'train_runtime': 19475.3256, 'train_samples_per_second': 1.515, 'train_steps_per_second': 0.379, 'total_flos': 6.417023025040589e+16, 'train_loss': 4.276153972395134, 'epoch': 2.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels will be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "dir_path = \"bart_squad_ft_climate/model\"\n",
    "model.save_pretrained(dir_path, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForQuestionAnswering(\n",
       "  (base_model): LoraModel(\n",
       "    (model): BartForQuestionAnswering(\n",
       "      (model): BartModel(\n",
       "        (shared): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "        (encoder): BartEncoder(\n",
       "          (embed_tokens): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "          (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "          (layers): ModuleList(\n",
       "            (0-11): 12 x BartEncoderLayer(\n",
       "              (self_attn): BartSdpaAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): BartDecoder(\n",
       "          (embed_tokens): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "          (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "          (layers): ModuleList(\n",
       "            (0-11): 12 x BartDecoderLayer(\n",
       "              (self_attn): BartSdpaAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (activation_fn): GELUActivation()\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (encoder_attn): BartSdpaAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (qa_outputs): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=1024, out_features=2, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=1024, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2096/2096 [02:31<00:00, 13.83it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(test_df.index):\n",
    "    question = test_df['question'][i]\n",
    "    text = test_df['context'][i]\n",
    "\n",
    "    inputs = tokenizer(question, text, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    answer_start_index = outputs.start_logits.argmax()\n",
    "    answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "    predict_answer_tokens = inputs.input_ids[0][answer_start_index:answer_end_index + 1]\n",
    "    answer_decoded = tokenizer.decode(predict_answer_tokens, skip_special_tokens=True)\n",
    "    test_df.at[i, 'bart_answer'] = answer_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge scores:\n",
      "{'rouge1': np.float64(0.44284512084597943), 'rouge2': np.float64(0.36824771949556834), 'rougeL': np.float64(0.40077112765096323), 'rougeLsum': np.float64(0.40091362499492345)}\n",
      "average semantic similarity:\n",
      "0.6182442903518677\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "predictions = test_df['bart_answer']\n",
    "references = test_df['answer']\n",
    "\n",
    "rouge_res = rouge.compute(predictions=predictions, references=references)\n",
    "print(f\"rouge scores:\\n{rouge_res}\")\n",
    "\n",
    "encoder_model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "candidate_embeddings = encoder_model.encode(predictions)\n",
    "reference_embeddings = encoder_model.encode(references)\n",
    "similarity = util.pairwise_cos_sim(candidate_embeddings, reference_embeddings)\n",
    "print(f\"average semantic similarity:\\n{torch.mean(similarity)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: The 9 percent reduction of rice in Bangladesh is attributed to what two variables?\n",
      "predicted answer:  anthropogenic climate change. using a combination of historical yield variability and ten climate projections, future anthropogenic climate change damages are estimated to reduce national rice production in bangladesh by about 9 percent to mid-century, and most of these losses are attributed in the analysis to flooding\n",
      "true answer: flooding damage and climate variability\n",
      "\n",
      "question: What kind of model of Bangladesh was had been used to estimate economic damages from historical climate variability and future anthropogenic climate change?\n",
      "predicted answer:  national level. for example, a dynamic economywide\n",
      "true answer: a dynamic economywide model\n",
      "\n",
      "question: What approach did Ahmed use to estimate how changes in climate variability might affect crop yields and poverty rates in Tanzania to the early 2030s\n",
      "predicted answer:  al., 2011). another example is the work of ahmed et al. (2011), who used a\n",
      "true answer: a modelling approach\n",
      "\n",
      "question: Where  height fluctuations are large?\n",
      "predicted answer: Where  height fluctuations are large?extreme sea level height fluctuations are also larger to the\n",
      "true answer: extreme sea level height fluctuations are also larger to the north, as a result of increasing storm intensities at the more northerly coastal locations\n",
      "\n",
      "question: How  non-tide sea levels are obtained?\n",
      "predicted answer:  northerly coastal locations (fig. 8 left). the non-tide sea levels are obtained by spectrally\n",
      "true answer: the non-tide sea levels are obtained by spectrally removing the tidal energy from the hourly tide gauge records (bromirski et al. 2003 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in test_df.index[:5]:\n",
    "    print(f\"question: {test_df['question'][i]}\")\n",
    "    print(f\"predicted answer: {test_df['bart_answer'][i]}\")\n",
    "    print(f\"true answer: {test_df['answer'][i]}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w266",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
